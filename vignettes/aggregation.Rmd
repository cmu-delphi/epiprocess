---
title: Aggregate signals over space and time
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Aggregate signals over space and time}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Aggregation, both time-wise and geo-wise, are common tasks when working with
epidemiological data sets. This vignette demonstrates how to carry out these
kinds of tasks with `epi_df` objects. We'll work with county-level reported
COVID-19 cases in MA and VT.

```{r, message = FALSE, eval= FALSE, warning= FALSE}
library(epidatr)
library(covidcast)
library(epiprocess)
library(dplyr)

# Use covidcast::county_census to get the county and state names
y <- covidcast::county_census %>%
  filter(STNAME %in% c("Massachusetts", "Vermont"), STNAME != CTYNAME) %>%
  select(geo_value = FIPS, county_name = CTYNAME, state_name = STNAME)

# Fetch only counties from Massachusetts and Vermont, then append names columns as well
x <- pub_covidcast(
  source = "jhu-csse",
  signals = "confirmed_incidence_num",
  geo_type = "county",
  time_type = "day",
  geo_values = paste(y$geo_value, collapse = ","),
  time_values = epirange(20200601, 20211231),
) %>%
  select(geo_value, time_value, cases = value) %>%
  full_join(y, by = "geo_value") %>%
  as_epi_df(as_of = as.Date("2024-03-20"))
```

The data contains 16,212 rows and 5 columns.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(epidatr)
library(covidcast)
library(epiprocess)
library(dplyr)

data(jhu_csse_county_level_subset)
x <- jhu_csse_county_level_subset
```

## Converting to `tsibble` format

For manipulating and wrangling time series data, the
[`tsibble`](https://tsibble.tidyverts.org/index.html) already provides a whole
bunch of useful tools. A tsibble object (formerly, of class `tbl_ts`) is
basically a tibble (data frame) but with two specially-marked columns: an
**index** column representing the time variable (defining an order from past to
present), and a **key** column identifying a unique observational unit for each
time point. In fact, the key can be made up of any number of columns, not just a
single one.

In an `epi_df` object, the index variable is `time_value`, and the key variable
is typically `geo_value` (though this need not always be the case: for example,
if we have an age group variable as another column, then this could serve as a
second key variable). The `epiprocess` package thus provides an implementation
of `as_tsibble()` for `epi_df` objects, which sets these variables according to
these defaults.

```{r, message = FALSE}
library(tsibble)

xt <- as_tsibble(x)
head(xt)
key(xt)
index(xt)
interval(xt)
```

We can also set the key variable(s) directly in a call to `as_tsibble()`.
Similar to SQL keys, if the key does not uniquely identify each time point (that
is, the key and index together do not not uniquely identify each row), then
`as_tsibble()` throws an error:

```{r, error = TRUE}
head(as_tsibble(x, key = "county_name"))
```

As we can see, there are duplicate county names between Massachusetts and
Vermont, which caused the error.

```{r, message = FALSE}
head(duplicates(x, key = "county_name"))
```

Keying by both county name and state name, however, does work:

```{r, message = FALSE}
head(as_tsibble(x, key = c("county_name", "state_name")))
```

## Detecting and filling time gaps

One of the major advantages of the `tsibble` package is its ability to handle
**implicit gaps** in time series data. In other words, it can infer what time
scale we're interested in (say, daily data), and detect apparent gaps (say, when
values are reported on January 1 and 3 but not January 2). We can subsequently
use functionality to make these missing entries explicit, which will generally
help avoid bugs in further downstream data processing tasks.

Let's first remove certain dates from our data set to create gaps:

```{r}
# First make geo value more readable for tables, plots, etc.
x <- x %>%
  mutate(geo_value = paste(
    substr(county_name, 1, nchar(county_name) - 7),
    name_to_abbr(state_name),
    sep = ", "
  )) %>%
  select(geo_value, time_value, cases)

xt <- as_tsibble(x) %>% filter(cases >= 3)
```

The functions `has_gaps()`, `scan_gaps()`, `count_gaps()` in the `tsibble`
package each provide useful summaries, in slightly different formats.

```{r}
head(has_gaps(xt))
head(scan_gaps(xt))
head(count_gaps(xt))
```

We can also visualize the patterns of missingness:

```{r, message = FALSE, warning = FALSE}
library(ggplot2)
theme_set(theme_bw())

ggplot(
  count_gaps(xt),
  aes(
    x = reorder(geo_value, desc(geo_value)),
    color = geo_value
  )
) +
  geom_linerange(aes(ymin = .from, ymax = .to)) +
  geom_point(aes(y = .from)) +
  geom_point(aes(y = .to)) +
  coord_flip() +
  labs(x = "County", y = "Date") +
  theme(legend.position = "none")
```

Using the `fill_gaps()` function from `tsibble`, we can replace all gaps by an
explicit value. The default is `NA`, but in the current case, where missingness
is not at random but rather represents a small value that was censored (only a
hypothetical with COVID-19 reports, but certainly a real phenomenon that occurs
in other signals), it is better to replace it by zero, which is what we do here.
(Other approaches, such as LOCF: last observation carried forward in time, could
be accomplished by first filling with `NA` values and then following up with a
second call to `tidyr::fill()`.)

```{r}
fill_gaps(xt, cases = 0) %>%
  head()
```

Note that the time series for Addison, VT only starts on August 27, 2020, even
though the original (uncensored) data set itself was drawn from a period that
went back to June 6, 2020. By setting `.full = TRUE`, we can at zero-fill over
the entire span of the observed (censored) data.

```{r}
xt_filled <- fill_gaps(xt, cases = 0, .full = TRUE)

head(xt_filled)
```

Explicit imputation for missingness (zero-filling in our case) can be important
for protecting against bugs in all sorts of downstream tasks. For example, even
something as simple as a 7-day trailing average is complicated by missingness.
The function `epi_slide()` looks for all rows within a window of 7 days anchored
on the right at the reference time point (when `.window_size = 7`).
But when some days in a given week are missing because they were censored
because they had small case counts, taking an average of the observed case
counts can be misleading and is unintentionally biased upwards. Meanwhile,
running `epi_slide()` on the zero-filled data brings these trailing averages
(appropriately) downwards, as we can see inspecting Plymouth, MA around July 1,
2021.

```{r}
xt %>%
  as_epi_df(as_of = as.Date("2024-03-20")) %>%
  group_by(geo_value) %>%
  epi_slide(cases_7dav = mean(cases), .window_size = 7) %>%
  ungroup() %>%
  filter(
    geo_value == "Plymouth, MA",
    abs(time_value - as.Date("2021-07-01")) <= 3
  ) %>%
  print(n = 7)

xt_filled %>%
  as_epi_df(as_of = as.Date("2024-03-20")) %>%
  group_by(geo_value) %>%
  epi_slide(cases_7dav = mean(cases), .window_size = 7) %>%
  ungroup() %>%
  filter(
    geo_value == "Plymouth, MA",
    abs(time_value - as.Date("2021-07-01")) <= 3
  ) %>%
  print(n = 7)
```

## Geographic aggregation

TODO

## Attribution
This document contains a dataset that is a modified part of the [COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University](https://github.com/CSSEGISandData/COVID-19) as [republished in the COVIDcast Epidata API](https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/jhu-csse.html). This data set is licensed under the terms of the [Creative Commons Attribution 4.0 International license](https://creativecommons.org/licenses/by/4.0/) by the Johns Hopkins University on behalf of its Center for Systems Science in Engineering. Copyright Johns Hopkins University 2020.

[From the COVIDcast Epidata API](https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/jhu-csse.html):
 These signals are taken directly from the JHU CSSE [COVID-19 GitHub repository](https://github.com/CSSEGISandData/COVID-19) without changes.

